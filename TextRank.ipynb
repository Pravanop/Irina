{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/rishabchitgo/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "nltk.download('punkt') \n",
    "import re\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DOWNLOAD GLOVE WORD EMBEDDINGS\n",
    "# !wget http://nlp.stanford.edu/data/glove.6B.zip\n",
    "# !unzip glove*.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract word vectors\n",
    "\n",
    "def wordEmbeddings():  \n",
    "    word_embeddings = {}\n",
    "    f = open('glove.6B.100d.txt', encoding='utf-8')\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        word_embeddings[word] = coefs\n",
    "    f.close()\n",
    "    return word_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TEXT PREPROCESSING\n",
    "def preprocess(sample):\n",
    "    # remove punctuations and special characters\n",
    "    clean_sample = re.sub('\\W+',' ', sample )\n",
    "    # make alphabets lowercase\n",
    "    clean_sample=clean_sample.lower()\n",
    "    #remove stopwords\n",
    "    clean_sample= remove_stopwords(clean_sample.split())\n",
    "    return clean_sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/rishabchitgo/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# function to remove stopwords\n",
    "nltk.download('stopwords')\n",
    "def remove_stopwords(sen):\n",
    "    from nltk.corpus import stopwords\n",
    "    stop_words = stopwords.words('english')\n",
    "    sen_new = \" \".join([i for i in sen if i not in stop_words])\n",
    "    return sen_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_list(clean_samples):\n",
    "    words=[]\n",
    "    for k in clean_samples.split():\n",
    "        words.append(k)\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def textRank(clean_samples):\n",
    "    words=word_list(clean_samples)\n",
    "    \n",
    "    # initializes similarity matrix\n",
    "    sim_mat = np.zeros([len(words), len(words)])\n",
    "    \n",
    "    from sklearn.metrics.pairwise import cosine_similarity\n",
    "    \n",
    "    #calculates similarity matrix for the sentence/word list\n",
    "    i=0\n",
    "    j=0\n",
    "    for a in words:\n",
    "        for b in words:\n",
    "            if i != j:\n",
    "                sim_mat[i][j] = cosine_similarity(word_embeddings[a].reshape(1,100), word_embeddings[b].reshape(1,100))[0,0]\n",
    "                j=j+1\n",
    "        i=i+1\n",
    "    \n",
    "    #PAGE RANK\n",
    "    import networkx as nx\n",
    "    nx_graph = nx.from_numpy_array(sim_mat)\n",
    "    scores = nx.pagerank(nx_graph)\n",
    "    #calulates most important words\n",
    "    ranked_words = sorted(((scores[i],s) for i,s in enumerate(words)), reverse=True)\n",
    "    return ranked_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'glove.6B.100d.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-2a80e1ad42f0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mword_embeddings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwordEmbeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m    \u001b[0;31m#excecution takes time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-3-faff819f41fc>\u001b[0m in \u001b[0;36mwordEmbeddings\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mwordEmbeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mword_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'glove.6B.100d.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'glove.6B.100d.txt'"
     ]
    }
   ],
   "source": [
    "word_embeddings=wordEmbeddings()    #excecution takes time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples=[\"What was the scoreline of the manu match yesterday night?\",\n",
    "         \"Read me my emails\",\n",
    "         \"Turn on the lights in the hall.\",\n",
    "         \"How many coronavirus cases are there in India?\"]\n",
    "#print(samples)\n",
    "for sample in samples:\n",
    "    clean_sample=preprocess(sample)\n",
    "    ranked_words=textRank(clean_sample)\n",
    "    print(sample)\n",
    "    for i in range(len(ranked_words)):\n",
    "        print(ranked_words[i][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "l=[]\n",
    "liscount=0\n",
    "for sample in samples:\n",
    "    clean_sample=preprocess(sample)\n",
    "    ranked_words=textRank(clean_sample)\n",
    "    temp=[]\n",
    "    \n",
    "    # creates a master list containing of smaller lists, each smaller list contains the words obtained from the question\n",
    "    for i in range(len(ranked_words)):\n",
    "        temp.append(ranked_words[i][1])\n",
    "        \n",
    "    l[liscount]=temp\n",
    "    liscount+=1\n",
    "    \n",
    "#Reversing the list\n",
    "def Reverse(lis):\n",
    "    lis.reverse()\n",
    "    return lis\n",
    "\n",
    "a=[]\n",
    "v=0\n",
    "for a in l:\n",
    "    Reverse(a)\n",
    "    l[v]=a\n",
    "    v+=1\n",
    "#Converting the reversed list to a dictionary along with the keys(i,e the question)\n",
    "final_dict={}\n",
    "var=0\n",
    "for sample in samples:\n",
    "    clean_sample=preprocess(sample)\n",
    "    ranked_words=textRank(clean_sample)\n",
    "    final_dict[sample]=l[var]\n",
    "    var+=1\n",
    "print(final_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
