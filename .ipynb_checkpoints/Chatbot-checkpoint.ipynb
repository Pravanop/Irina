{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq2seq_model_builder(HIDDEN_DIM=300):\n",
    "    \n",
    "    encoder_inputs = Input(shape=(MAX_LEN, ), dtype='int32',)\n",
    "    encoder_embedding = embed_layer(encoder_inputs)\n",
    "    encoder_LSTM = LSTM(HIDDEN_DIM, return_state=True)\n",
    "    encoder_outputs, state_h, state_c = encoder_LSTM(encoder_embedding)\n",
    "    \n",
    "    decoder_inputs = Input(shape=(MAX_LEN, ), dtype='int32',)\n",
    "    decoder_embedding = embed_layer(decoder_inputs)\n",
    "    decoder_LSTM = LSTM(HIDDEN_DIM, return_state=True, return_sequences=True)\n",
    "    decoder_outputs, _, _ = decoder_LSTM(decoder_embedding, initial_state=[state_h, state_c])\n",
    "    \n",
    "    # dense_layer = Dense(VOCAB_SIZE, activation='softmax')\n",
    "    outputs = TimeDistributed(Dense(VOCAB_SIZE, activation='softmax'))(decoder_outputs)\n",
    "    model = Model([encoder_inputs, decoder_inputs], outputs)\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = seq2seq_model_builder(HIDDEN_DIM=300)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    '''Clean text by removing unnecessary characters and altering the format of words.'''\n",
    "\n",
    "    text = text.lower()\n",
    "    \n",
    "    text = re.sub(r\"i'm\", \"i am\", text)\n",
    "    text = re.sub(r\"he's\", \"he is\", text)\n",
    "    text = re.sub(r\"she's\", \"she is\", text)\n",
    "    text = re.sub(r\"it's\", \"it is\", text)\n",
    "    text = re.sub(r\"that's\", \"that is\", text)\n",
    "    text = re.sub(r\"what's\", \"that is\", text)\n",
    "    text = re.sub(r\"where's\", \"where is\", text)\n",
    "    text = re.sub(r\"how's\", \"how is\", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will\", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have\", text)\n",
    "    text = re.sub(r\"\\'re\", \" are\", text)\n",
    "    text = re.sub(r\"\\'d\", \" would\", text)\n",
    "    text = re.sub(r\"\\'re\", \" are\", text)\n",
    "    text = re.sub(r\"won't\", \"will not\", text)\n",
    "    text = re.sub(r\"can't\", \"cannot\", text)\n",
    "    text = re.sub(r\"n't\", \" not\", text)\n",
    "    text = re.sub(r\"n'\", \"ng\", text)\n",
    "    text = re.sub(r\"'bout\", \"about\", text)\n",
    "    text = re.sub(r\"'til\", \"until\", text)\n",
    "    text = re.sub(r\"[-()\\\"#/@;:<>{}`+=~|.!?,]\", \"\", text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tagger(decoder_input_sentence):\n",
    "  bos = \"<BOS> \"\n",
    "  eos = \" <EOS>\"\n",
    "  final_target = [bos + text + eos for text in decoder_input_sentence] \n",
    "  return final_target\n",
    "\n",
    "decoder_inputs = tagger(decoder_input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "def vocab_creater(text_lists, VOCAB_SIZE):\n",
    "\n",
    "    tokenizer = Tokenizer(num_words=VOCAB_SIZE)\n",
    "    tokenizer.fit_on_texts(text_lists)\n",
    "    dictionary = tokenizer.word_index\n",
    "\n",
    "    word2idx = {}\n",
    "    idx2word = {}\n",
    "    for k, v in dictionary.items():\n",
    "        if v < VOCAB_SIZE:\n",
    "            word2idx[k] = v\n",
    "            index2word[v] = k\n",
    "        if v >= VOCAB_SIZE-1:\n",
    "            continue\n",
    "\n",
    "    return word2idx, idx2word\n",
    "\n",
    "word2idx, idx2word = vocab_creater(text_lists=encoder_input_text+decoder_input_text, VOCAB_SIZE=14999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "VOCAB_SIZE = 14999\n",
    "\n",
    "def text2seq(encoder_text, decoder_text, VOCAB_SIZE):\n",
    "\n",
    "    tokenizer = Tokenizer(num_words=VOCAB_SIZE)\n",
    "    encoder_sequences = tokenizer.texts_to_sequences(encoder_text)\n",
    "    decoder_sequences = tokenizer.texts_to_sequences(decoder_text)\n",
    "\n",
    "    return encoder_sequences, decoder_sequences\n",
    "\n",
    "encoder_sequences, decoder_sequences = text2seq(encoder_text, decoder_text, VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "def padding(encoder_sequences, decoder_sequences, MAX_LEN):\n",
    "  \n",
    "    encoder_input_data = pad_sequences(encoder_sequences, maxlen=MAX_LEN, dtype='int32', padding='post', truncating='post')\n",
    "    decoder_input_data = pad_sequences(decoder_sequences, maxlen=MAX_LEN, dtype='int32', padding='post', truncating='post')\n",
    "\n",
    "    return encoder_input_data, decoder_input_data\n",
    "\n",
    "encoder_input_data, decoder_input_data = padding(encoder_sequences, decoder_sequences, MAX_LEN):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GLOVE_DIR = path for glove.6B.100d.txt\n",
    "def glove_100d_dictionary(GLOVE_DIR):\n",
    "    embeddings_index = {}\n",
    "    f = open(os.path.join(GLOVE_DIR, 'glove.6B.100d.txt'))\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "    f.close()\n",
    "    return embeddings_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this time: embedding_dimention = 100d\n",
    "def embedding_matrix_creater(embedding_dimention):\n",
    "    embedding_matrix = np.zeros((len(word_index) + 1, embedding_dimention))\n",
    "    for word, i in word_index.items():\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "          # words not found in embedding index will be all-zeros.\n",
    "          embedding_matrix[i] = embedding_vector\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embedding_layer_creater(VOCAB_SIZE, EMBEDDING_DIM, MAX_LEN, embedding_matrix):\n",
    "  \n",
    "  embedding_layer = Embedding(input_dim = VOCAB_SIZE, \n",
    "                              output_dim = EMBEDDING_DIM,\n",
    "                              input_length = MAX_LEN,\n",
    "                              weights = [embedding_matrix],\n",
    "                              trainable = False)\n",
    "    return embedding_layer\n",
    "\n",
    "embedding_layer = embedding_layer_creater(VOCAB_SIZE, EMBEDDING_DIM, MAX_LEN, embedding_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# MAX_LEN = 20\n",
    "# num_samples = len(encoder_sequences)\n",
    "# VOCAB_SIZE = 15000\n",
    "\n",
    "def decoder_output_creater(decoder_input_data, num_samples, MAX_LEN, VOCAB_SIZE):\n",
    "  \n",
    "  decoder_output_data = np.zeros((num_samples, MAX_LEN, VOCAB_SIZE), dtype=\"float32\")\n",
    "\n",
    "    for i, seqs in enumerate(decoder_input_data):\n",
    "        for j, seq in enumerate(seqs):\n",
    "            if j > 0:\n",
    "                decoder_output_data[i][j][seq] = 1.\n",
    "    print(decoder_output_data.shape)\n",
    "\n",
    "    return decoder_output_data\n",
    "\n",
    "decoder_output_data = decoder_output_creater(decoder_input_data, num_samples, MAX_LEN, VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def data_spliter(encoder_input_data, decoder_input_data, test_size1=0.2, test_size2=0.3):\n",
    "  \n",
    "  en_train, en_test, de_train, de_test = train_test_split(encoder_input_data, decoder_input_data, test_size=test_size1)\n",
    "    en_train, en_val, de_train, de_val = train_test_split(en_train, de_train, test_size=test_size2)\n",
    "\n",
    "    return en_train, en_val, en_test, de_train, de_val, de_test\n",
    "\n",
    "en_train, en_val, en_test, de_train, de_val, de_test = data_spliter(encoder_input_data, decoder_input_data)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
