{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Sandesh\n",
      "[nltk_data]     Rangreji\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "nltk.download('punkt') \n",
    "import re\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DOWNLOAD GLOVE WORD EMBEDDINGS\n",
    "# !wget http://nlp.stanford.edu/data/glove.6B.zip\n",
    "# !unzip glove*.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract word vectors\n",
    "\n",
    "def wordEmbeddings():  \n",
    "    word_embeddings = {}\n",
    "    f = open('glove.6B.100d.txt', encoding='utf-8')\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        word_embeddings[word] = coefs\n",
    "    f.close()\n",
    "    return word_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TEXT PREPROCESSING\n",
    "def preprocess(sample):\n",
    "    # remove punctuations and special characters\n",
    "    clean_sample = re.sub('\\W+',' ', sample )\n",
    "    # make alphabets lowercase\n",
    "    clean_sample=clean_sample.lower()\n",
    "    #remove stopwords\n",
    "    clean_sample= remove_stopwords(clean_sample.split())\n",
    "    return clean_sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Sandesh\n",
      "[nltk_data]     Rangreji\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# function to remove stopwords\n",
    "nltk.download('stopwords')\n",
    "def remove_stopwords(sen):\n",
    "    from nltk.corpus import stopwords\n",
    "    stop_words = stopwords.words('english')\n",
    "    sen_new = \" \".join([i for i in sen if i not in stop_words])\n",
    "    return sen_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_list(clean_samples):\n",
    "    words=[]\n",
    "    for k in clean_samples.split():\n",
    "        words.append(k)\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def textRank(clean_samples):\n",
    "    words=word_list(clean_samples)\n",
    "    \n",
    "    # initializes similarity matrix\n",
    "    sim_mat = np.zeros([len(words), len(words)])\n",
    "    \n",
    "    from sklearn.metrics.pairwise import cosine_similarity\n",
    "    \n",
    "    #calculates similarity matrix for the sentence/word list\n",
    "    i=0\n",
    "    j=0\n",
    "    for a in words:\n",
    "        for b in words:\n",
    "            if i != j:\n",
    "                sim_mat[i][j] = cosine_similarity(word_embeddings[a].reshape(1,100), word_embeddings[b].reshape(1,100))[0,0]\n",
    "                j=j+1\n",
    "        i=i+1\n",
    "    \n",
    "    #PAGE RANK\n",
    "    import networkx as nx\n",
    "    nx_graph = nx.from_numpy_array(sim_mat)\n",
    "    scores = nx.pagerank(nx_graph)\n",
    "    #calulates most important words\n",
    "    ranked_words = sorted(((scores[i],s) for i,s in enumerate(words)), reverse=True)\n",
    "    return ranked_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "word_embeddings=wordEmbeddings()    #excecution takes time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What was the scoreline of the manu match yesterday night?\n",
      "match\n",
      "manu\n",
      "yesterday\n",
      "night\n",
      "scoreline\n",
      "Read me my emails\n",
      "read\n",
      "emails\n",
      "Turn on the lights in the hall.\n",
      "lights\n",
      "turn\n",
      "hall\n",
      "How many coronavirus cases are there in India?\n",
      "cases\n",
      "coronavirus\n",
      "india\n",
      "many\n"
     ]
    }
   ],
   "source": [
    "samples=[\"What was the scoreline of the manu match yesterday night?\",\n",
    "         \"Read me my emails\",\n",
    "         \"Turn on the lights in the hall.\",\n",
    "         \"How many coronavirus cases are there in India?\"]\n",
    "#print(samples)\n",
    "for sample in samples:\n",
    "    clean_sample=preprocess(sample)\n",
    "    ranked_words=textRank(clean_sample)\n",
    "    print(sample)\n",
    "    for i in range(len(ranked_words)):\n",
    "      print(ranked_words[i][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
